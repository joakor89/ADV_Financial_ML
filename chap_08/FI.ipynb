{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "335cc01d-ca33-4caa-8a0c-0502cfe2b6ce",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9f97ab-eb66-488c-b996-7a2d741d5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomness\n",
    "import random\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "%matplotlib inline\n",
    "\n",
    "# Date & Time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Typing\n",
    "from typing import Tuple, List, Dict, Union, Optional, Any, Generator\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, plot_roc_curve\n",
    "\n",
    "# Scientific Statistical Python\n",
    "from scipy.stats import jarque_bera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1869990-13d1-4072-9152-682d42a069d7",
   "metadata": {},
   "source": [
    "## Feature Importance with Substitution Effects\n",
    "\n",
    "### Mean Decrease Impurity (MDI)\n",
    "#### MDI Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b2bfdf-107f-4c07-9696-e49be37141b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp_MDI(fit: Any, featNames: np.ndarray) -> pd.DataFrame:\n",
    "    df0 = {i: tree.feature_importances_ for i, tree in enumerate(fit.estimators_)}\n",
    "    df0 = pd.DataFrame.from_dict(df0, orient='index')\n",
    "    df0.columns = featNames\n",
    "    df0 = df0.replace(0, np.nan)    # because max_features=1\n",
    "    imp = pd.concat({'mean': df0.mean(), 'std': df0.std() * df0.shape[0] ** (-0.5)}, axis=1)\n",
    "    imp /= imp['mean'].sum()\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bbdb6c-82dc-4bf4-9cc2-e382fcc4284e",
   "metadata": {},
   "source": [
    "### Mean Decrease Accuracy\n",
    "#### MDA Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2ce91d-8748-4d64-adc1-9d0022df70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp_MDA(\n",
    "    clf: Any, X: pd.DataFrame, y: pd.Series, cv: int, sample_weight: pd.Series,\n",
    "    t1: pd.Series, pctEmbargo: float, scoring: str = 'neg_log_loss') -> Tuple[pd.DataFrame, float]:\n",
    "\n",
    "    if scoring not in ['neg_log_loss', 'accuracy']:\n",
    "        raise Exception('wrong scoring method.')\n",
    "    cvGen = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)    # purged cv\n",
    "    scr0, scr1 = pd.Series(), pd.DataFrame(columns=X.columns, dtype=object)\n",
    "    \n",
    "    for i, (train, test) in enumerate(cvGen.split(X=X)):\n",
    "        X0, y0, w0 = X.iloc[train, :], y.iloc[train], sample_weight.iloc[train]\n",
    "        X1, y1, w1 = X.iloc[test, :], y.iloc[test], sample_weight.iloc[test]\n",
    "        fit = clf.fit(X=X0, y=y0, sample_weight=w0.values)\n",
    "        if scoring == 'neg_log_loss':\n",
    "            prob = fit.predict_proba(X1)\n",
    "            scr0.loc[i] = -log_loss(y1, prob, sample_weight=w1.values, labels=clf.classes_)\n",
    "        else:\n",
    "            pred = fit.predict(X1)\n",
    "            scr0.loc[i] = accuracy_score(y1, pred, sample_weight=w1.values)\n",
    "        for j in X.columns:\n",
    "            X1_ = X1.copy(deep=True)\n",
    "            np.random.shuffle(X1_[j].values)    # permutation of a single column\n",
    "            if scoring == 'neg_log_loss':\n",
    "                prob = fit.predict_proba(X1_)\n",
    "                scr1.loc[i, j] = -log_loss(y1, prob, sample_weight=w1.values, labels=clf.classes_)\n",
    "            else:\n",
    "                pred = fit.predict(X1_)\n",
    "                scr1.loc[i, j] = accuracy_score(y1, pred, sample_weight=w1.values)\n",
    "    imp = (-scr1).add(scr0, axis=0)\n",
    "    if scoring == 'neg_log_loss':\n",
    "        imp = imp / -scr1\n",
    "    else:\n",
    "        imp = imp / (1.0 - scr1)\n",
    "    imp = pd.concat({'mean': imp.mean(), 'std': imp.std() * imp.shape[0] ** (-0.5)}, axis=1)\n",
    "    return imp, scr0.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde51465-73ce-49c4-b1b8-cc1d80426a08",
   "metadata": {},
   "source": [
    "## Feature Importance without Substitution Effects\n",
    "\n",
    "### Single Feature Importance\n",
    "#### SFI Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "017d498a-8bc3-4905-b971-177b81d10855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0444a5-6361-4abc-9a62-756fbbfe8492",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PurgedKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maux_feat_imp_SFI\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     featNames: np\u001b[38;5;241m.\u001b[39mndarray, clf: Any, trnsX: pd\u001b[38;5;241m.\u001b[39mDataFrame, cont: pd\u001b[38;5;241m.\u001b[39mDataFrame, scoring: \u001b[38;5;28mstr\u001b[39m, cvGen: PurgedKFold) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m      4\u001b[0m     imp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m featName \u001b[38;5;129;01min\u001b[39;00m featNames:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PurgedKFold' is not defined"
     ]
    }
   ],
   "source": [
    "def aux_feat_imp_SFI(\n",
    "    featNames: np.ndarray, clf: Any, trnsX: pd.DataFrame, cont: pd.DataFrame, scoring: str, cvGen: PurgedKFold) -> pd.DataFrame:\n",
    "    \n",
    "    imp = pd.DataFrame(columns=['mean', 'std'], dtype=object)\n",
    "    for featName in featNames:\n",
    "        df0 = cvScore(clf, X=trnsX[[featName]], y=cont['bin'], sample_weight=cont['w'], scoring=scoring, cvGen=cvGen)\n",
    "        imp.loc[featName, 'mean'] = df0.mean()\n",
    "        imp.loc[featName, 'std'] = df0.std() * df0.shape[0] ** (-0.5)\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f0a49-21e7-49f6-a0c2-a7a35e50d018",
   "metadata": {},
   "source": [
    "### Orthogonal Features\n",
    "#### Orthogonal Features Computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad9f951-7ba9-46d9-83ac-d61e89bf6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eVec(dot: np.ndarray, varThres: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    eVal, eVec = np.linalg.eigh(dot)\n",
    "    idx = eVal.argsort()[::-1]    # arguments for sorting eVal desc\n",
    "    eVal, eVec = eVal[idx], eVec[:, idx]\n",
    "    eVal = pd.Series(eVal, index=['PC_' + str(i + 1) for i in range(eVal.shape[0])])\n",
    "    eVec = pd.DataFrame(eVec, index=dot.index, columns=eVal.index)\n",
    "    eVec = eVec.loc[:, eVal.index]\n",
    "    cumVar = eVal.cumsum() / eVal.sum()\n",
    "    dim = cumVar.values.searchsorted(varThres)\n",
    "    eVal, eVec = eVal.iloc[:dim + 1], eVec.iloc[:, :dim + 1]\n",
    "    return eVal, eVec\n",
    "\n",
    "\n",
    "def ortho_feats(dfX: pd.DataFrame, varThres: float = 0.95) -> pd.DataFrame:\n",
    "    dfZ = dfX.sub(dfX.mean(), axis=1).div(dfX.std(), axis=1)    # standardize\n",
    "    dot = pd.DataFrame(np.dot(dfZ.T, dfZ), index=dfX.columns, columns=dfX.columns)\n",
    "    eVal, eVec = get_eVec(dot, varThres)\n",
    "    dfP = np.dot(dfZ, eVec)\n",
    "    return pd.DataFrame(dfP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc1c61-bcfe-4e70-b4da-37e5c9ef08a3",
   "metadata": {},
   "source": [
    "## Experiment with Synthetic Data\n",
    "\n",
    "#### Creating a Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e61223f-f472-41d5-a0c4-ff9902ebb5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(n_features: int = 40, n_informative: int = 10,\n",
    "                  n_redundant: int = 10, n_samples: int = 10000) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    trnsX, cont = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative,\n",
    "                                      n_redundant=n_redundant, random_state=0, shuffle=False)\n",
    "    trnsX, cont = pd.DataFrame(trnsX), pd.Series(cont).to_frame('bin')\n",
    "    df0 = ['I_' + str(i) for i in range(n_informative)] + ['R_' + str(i) for i in range(n_redundant)]\n",
    "    df0 += ['N_' + str(i) for i in range(n_features - len(df0))]\n",
    "    trnsX.columns = df0\n",
    "    cont['w'] = 1.0 / cont.shape[0]\n",
    "    cont['t1'] = pd.Series(cont.index, index=cont.index)\n",
    "    return trnsX, cont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f6352-32b1-497b-8573-5fad657a79b3",
   "metadata": {},
   "source": [
    "#### Calling Feature Importance for any Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b72a117f-07f6-415e-a104-87f8da045bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_importance(\n",
    "    trnsX: pd.DataFrame, cont: pd.DataFrame, n_estimators: int = 100, cv: int = 10,\n",
    "    max_samples: float = 1.0, pctEmbargo: float = 0.0, scoring: str = 'accuracy',\n",
    "    method: str = 'SFI', min_weight_fraction_leaf: float = 0.0, ensemble: str = 'bagging') -> Tuple[pd.DataFrame, float, float]:\n",
    "    \n",
    "    if ensemble == 'bagging':\n",
    "        clf = DecisionTreeClassifier(criterion='entropy', max_features=1, class_weight='balanced',\n",
    "                                     min_weight_fraction_leaf=min_weight_fraction_leaf)\n",
    "        clf = BaggingClassifier(base_estimator=clf, n_estimators=n_estimators, max_features=1.0,\n",
    "                                max_samples=max_samples, oob_score=True, n_jobs=-1)\n",
    "    else:\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, criterion='entropy',\n",
    "                                     min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                                     max_features=1, oob_score=True, n_jobs=-1, max_samples=max_samples)\n",
    "    fit = clf.fit(X=trnsX, y=cont['bin'], sample_weight=cont['w'].values)\n",
    "    oob = fit.oob_score_\n",
    "    \n",
    "    if method == 'MDI':\n",
    "        imp = feat_imp_MDI(fit, featNames=trnsX.columns)\n",
    "        oos = cvScore(clf, X=trnsX, y=cont['bin'], cv=cv, sample_weight=cont['w'], t1=cont['t1'],\n",
    "                      pctEmbargo=pctEmbargo, scoring=scoring).mean()\n",
    "        \n",
    "    elif method=='MDA':\n",
    "        imp, oos = feat_imp_MDA(clf, X=trnsX, y=cont['bin'], cv=cv, sample_weight=cont['w'], t1=cont['t1'],\n",
    "                                pctEmbargo=pctEmbargo, scoring=scoring)\n",
    "        \n",
    "    elif method=='SFI':\n",
    "        cvGen = PurgedKFold(n_splits=cv, t1=cont['t1'], pctEmbargo=pctEmbargo)\n",
    "        oos = cvScore(clf, X=trnsX, y=cont['bin'], sample_weight=cont['w'],\n",
    "                      scoring=scoring, cvGen=cvGen).mean()\n",
    "        imp = aux_feat_imp_SFI(featNames=trnsX.columns, clf=clf, trnsX=trnsX, cont=cont,\n",
    "                               scoring=scoring, cvGen=cvGen)\n",
    "    \n",
    "    return imp, oob, oos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f681b-f116-4f64-8089-3bd89ef79781",
   "metadata": {},
   "source": [
    "#### Calling All Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f319334-3485-4926-8813-30914e45ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(n_features: int = 40, n_informative: int = 10, n_redundant: int = 10,\n",
    "              n_estimators: int = 100, n_samples: int = 10000, cv: int = 10) -> pd.DataFrame:\n",
    "    trnsX, cont = get_test_data(n_features, n_informative, n_redundant, n_samples)\n",
    "    dict0 = {'minWLeaf': [0.0], 'scoring': ['accuracy'], 'method': ['MDI', 'MDA', 'SFI'], 'max_samples': [1.0]}\n",
    "    jobs, out = (dict(zip(dict0, i)) for i in product(*dict0.values())), []\n",
    "    \n",
    "    for job in jobs:\n",
    "        job['simNum'] = job['method'] +'_' + job['scoring'] + '_' + '%.2f'%job['minWLeaf'] + \\\n",
    "                        '_' + str(job['max_samples'])\n",
    "        print(job['simNum'])\n",
    "        imp, oob, oos = feat_importance(trnsX=trnsX, cont=cont, n_estimators=n_estimators,\n",
    "                                        cv=cv, max_samples=job['max_samples'], scoring=job['scoring'],\n",
    "                                        method=job['method'])\n",
    "        plot_feat_importance(imp=imp, oob=oob, oos=oos, method=job['method'],\n",
    "                             tag='test_func', simNum=job['simNum'])\n",
    "        df0 = imp[['mean']] / imp['mean'].abs().sum()\n",
    "        df0['type'] = [i[0] for i in df0.index]\n",
    "        df0 = df0.groupby('type')['mean'].sum().to_dict()\n",
    "        df0.update({'oob': oob, 'oos': oos})\n",
    "        df0.update(job)\n",
    "        out.append(df0)\n",
    "    \n",
    "    out = pd.DataFrame(out).sort_values(['method', 'scoring', 'minWLeaf', 'max_samples'])\n",
    "    out = out[['method', 'scoring', 'minWLeaf', 'max_samples', 'I', 'R', 'N', 'oob', 'oos']]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584e772-055c-4daf-9ec4-b274624efd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
