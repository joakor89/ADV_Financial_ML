{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "335cc01d-ca33-4caa-8a0c-0502cfe2b6ce",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9f97ab-eb66-488c-b996-7a2d741d5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomness\n",
    "import random\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "%matplotlib inline\n",
    "\n",
    "# Date & Time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Typing\n",
    "from typing import Tuple, List, Dict, Union, Optional, Any, Generator\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, plot_roc_curve\n",
    "\n",
    "# Scientific Statistical Python\n",
    "from scipy.stats import jarque_bera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1869990-13d1-4072-9152-682d42a069d7",
   "metadata": {},
   "source": [
    "## Feature Importance with Substitution Effects\n",
    "\n",
    "### Mean Decrease Impurity (MDI)\n",
    "#### MDI Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b2bfdf-107f-4c07-9696-e49be37141b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp_MDI(fit: Any, featNames: np.ndarray) -> pd.DataFrame:\n",
    "    df0 = {i: tree.feature_importances_ for i, tree in enumerate(fit.estimators_)}\n",
    "    df0 = pd.DataFrame.from_dict(df0, orient='index')\n",
    "    df0.columns = featNames\n",
    "    df0 = df0.replace(0, np.nan)    # because max_features=1\n",
    "    imp = pd.concat({'mean': df0.mean(), 'std': df0.std() * df0.shape[0] ** (-0.5)}, axis=1)\n",
    "    imp /= imp['mean'].sum()\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bbdb6c-82dc-4bf4-9cc2-e382fcc4284e",
   "metadata": {},
   "source": [
    "### Mean Decrease Accuracy\n",
    "#### MDA Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2ce91d-8748-4d64-adc1-9d0022df70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp_MDA(\n",
    "    clf: Any, X: pd.DataFrame, y: pd.Series, cv: int, sample_weight: pd.Series,\n",
    "    t1: pd.Series, pctEmbargo: float, scoring: str = 'neg_log_loss') -> Tuple[pd.DataFrame, float]:\n",
    "\n",
    "    if scoring not in ['neg_log_loss', 'accuracy']:\n",
    "        raise Exception('wrong scoring method.')\n",
    "    cvGen = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)    # purged cv\n",
    "    scr0, scr1 = pd.Series(), pd.DataFrame(columns=X.columns, dtype=object)\n",
    "    \n",
    "    for i, (train, test) in enumerate(cvGen.split(X=X)):\n",
    "        X0, y0, w0 = X.iloc[train, :], y.iloc[train], sample_weight.iloc[train]\n",
    "        X1, y1, w1 = X.iloc[test, :], y.iloc[test], sample_weight.iloc[test]\n",
    "        fit = clf.fit(X=X0, y=y0, sample_weight=w0.values)\n",
    "        if scoring == 'neg_log_loss':\n",
    "            prob = fit.predict_proba(X1)\n",
    "            scr0.loc[i] = -log_loss(y1, prob, sample_weight=w1.values, labels=clf.classes_)\n",
    "        else:\n",
    "            pred = fit.predict(X1)\n",
    "            scr0.loc[i] = accuracy_score(y1, pred, sample_weight=w1.values)\n",
    "        for j in X.columns:\n",
    "            X1_ = X1.copy(deep=True)\n",
    "            np.random.shuffle(X1_[j].values)    # permutation of a single column\n",
    "            if scoring == 'neg_log_loss':\n",
    "                prob = fit.predict_proba(X1_)\n",
    "                scr1.loc[i, j] = -log_loss(y1, prob, sample_weight=w1.values, labels=clf.classes_)\n",
    "            else:\n",
    "                pred = fit.predict(X1_)\n",
    "                scr1.loc[i, j] = accuracy_score(y1, pred, sample_weight=w1.values)\n",
    "    imp = (-scr1).add(scr0, axis=0)\n",
    "    if scoring == 'neg_log_loss':\n",
    "        imp = imp / -scr1\n",
    "    else:\n",
    "        imp = imp / (1.0 - scr1)\n",
    "    imp = pd.concat({'mean': imp.mean(), 'std': imp.std() * imp.shape[0] ** (-0.5)}, axis=1)\n",
    "    return imp, scr0.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde51465-73ce-49c4-b1b8-cc1d80426a08",
   "metadata": {},
   "source": [
    "## Feature Importance without Substitution Effects\n",
    "\n",
    "### Single Feature Importance\n",
    "#### SFI Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "017d498a-8bc3-4905-b971-177b81d10855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0444a5-6361-4abc-9a62-756fbbfe8492",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PurgedKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maux_feat_imp_SFI\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     featNames: np\u001b[38;5;241m.\u001b[39mndarray, clf: Any, trnsX: pd\u001b[38;5;241m.\u001b[39mDataFrame, cont: pd\u001b[38;5;241m.\u001b[39mDataFrame, scoring: \u001b[38;5;28mstr\u001b[39m, cvGen: PurgedKFold) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m      4\u001b[0m     imp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m featName \u001b[38;5;129;01min\u001b[39;00m featNames:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PurgedKFold' is not defined"
     ]
    }
   ],
   "source": [
    "def aux_feat_imp_SFI(\n",
    "    featNames: np.ndarray, clf: Any, trnsX: pd.DataFrame, cont: pd.DataFrame, scoring: str, cvGen: PurgedKFold) -> pd.DataFrame:\n",
    "    \n",
    "    imp = pd.DataFrame(columns=['mean', 'std'], dtype=object)\n",
    "    for featName in featNames:\n",
    "        df0 = cvScore(clf, X=trnsX[[featName]], y=cont['bin'], sample_weight=cont['w'], scoring=scoring, cvGen=cvGen)\n",
    "        imp.loc[featName, 'mean'] = df0.mean()\n",
    "        imp.loc[featName, 'std'] = df0.std() * df0.shape[0] ** (-0.5)\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd192d-11e1-49ad-b706-22cd2fb3c44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
