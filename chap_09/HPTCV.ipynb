{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4864b0a3-2caa-419f-ae38-555e8cdfc8e4",
   "metadata": {},
   "source": [
    "# Hyper-parameter Tuning with Cross-Validation\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6bdb42e-198c-420c-833d-dbe5b7fe2d9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_roc_curve' from 'sklearn.metrics' (/Users/isisromero/anaconda3/envs/OOP/lib/python3.11/site-packages/sklearn/metrics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, plot_roc_curve\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Scientific Statistical Python\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jarque_bera\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_roc_curve' from 'sklearn.metrics' (/Users/isisromero/anaconda3/envs/OOP/lib/python3.11/site-packages/sklearn/metrics/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Randomness\n",
    "import random\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "%matplotlib inline\n",
    "\n",
    "# Date & Time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Typing\n",
    "from typing import Tuple, List, Dict, Union, Optional, Any, Generator\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, plot_roc_curve\n",
    "\n",
    "# Scientific Statistical Python\n",
    "from scipy.stats import jarque_bera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9ec62a-a553-46e1-99e1-b9ccbf81b457",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMyPipeline\u001b[39;00m(Pipeline):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Augmentation of sklearn Pipeline class that allows to pass 'sample_weight' to 'fit' method.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mself\u001b[39m, X: pd\u001b[38;5;241m.\u001b[39mDataFrame, y: pd\u001b[38;5;241m.\u001b[39mSeries, sample_weight: Optional[pd\u001b[38;5;241m.\u001b[39mSeries] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params\n\u001b[1;32m      7\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMyPipeline\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "class MyPipeline(Pipeline):\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, sample_weight: Optional[pd.Series] = None, **fit_params) -> 'MyPipeline':\n",
    "        if sample_weight is not None:\n",
    "            fit_params[self.steps[-1][0] + '__sample_weight'] = sample_weight\n",
    "        return super().fit(X, y, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02351a69-eb99-4389-9bd5-2f06c3f036e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_hyper_fit_base(\n",
    "    feat: pd.DataFrame, lbl: pd.Series, t1: pd.Series, pipe_clf: Any, param_grid: Dict[str, list],\n",
    "    cv: int = 3, bagging: list = [0, None, 1.0], n_jobs: int = -1, pctEmbargo: float = 0.0, **fit_params) -> Any:\n",
    "   \n",
    "    if set(lbl.values) == {0, 1}:\n",
    "        scoring='f1'    # f1 for meta-labeling\n",
    "    else:\n",
    "        scoring='neg_log_loss'    # symmetric towards all cases\n",
    "    inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)    # purged\n",
    "    gs=GridSearchCV(estimator=pipe_clf ,param_grid=param_grid, scoring=scoring, cv=inner_cv, n_jobs=n_jobs)\n",
    "    gs = gs.fit(feat, lbl, **fit_params).best_estimator_    # pipeline\n",
    "    if bagging[1] is not None and bagging[1] > 0:\n",
    "        gs = BaggingClassifier(base_estimator=MyPipeline(gs.steps), n_estimators=int(bagging[0]),\n",
    "                               max_samples=float(bagging[1]), max_features=float(bagging[2]), n_jobs=n_jobs)\n",
    "        gs = gs.fit(feat, lbl, sample_weight=fit_params[gs.base_estimator.steps[-1][0]+'__sample_weight'])\n",
    "        gs = Pipeline([('bag', gs)])\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e104781-7228-484a-a823-d3564a4f37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clf_hyper_fit(\n",
    "    feat: pd.DataFrame, lbl: pd.Series, t1: pd.Series, pipe_clf: Any, param_grid: Dict[str, list],\n",
    "    cv: int = 3, bagging: list = [0, None, 1.0], rndSearchIter: int = 0,\n",
    "    n_jobs: int = -1, pctEmbargo: float = 0.0, **fit_params) -> Any:\n",
    "    \n",
    "    if set(lbl.values) == {0, 1}:\n",
    "        scoring='f1'    # f1 for meta-labeling\n",
    "    else:\n",
    "        scoring='neg_log_loss'    # symmetric towards all cases\n",
    "    inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)    # purged\n",
    "    \n",
    "    if rndSearchIter == 0:\n",
    "        gs = GridSearchCV(estimator=pipe_clf, param_grid=param_grid, scoring=scoring, cv=inner_cv, n_jobs=n_jobs)\n",
    "    else:\n",
    "        gs = RandomizedSearchCV(estimator=pipe_clf, param_distributions=param_grid, scoring=scoring,\n",
    "                                cv=inner_cv, n_jobs=n_jobs, n_iter=rndSearchIter)\n",
    "    gs = gs.fit(feat, lbl, **fit_params).best_estimator_    # pipeline\n",
    "    \n",
    "    if bagging[1] is not None and bagging[1] > 0:\n",
    "        gs = BaggingClassifier(base_estimator=MyPipeline(gs.steps), n_estimators=int(bagging[0]),\n",
    "                               max_samples=float(bagging[1]), max_features=float(bagging[2]), n_jobs=n_jobs)\n",
    "        gs = gs.fit(feat, lbl, sample_weight=fit_params[gs.base_estimator.steps[-1][0]+'__sample_weight'])\n",
    "        gs = Pipeline([('bag', gs)])\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc3bdd-9b3b-4a3f-bc35-f135ac11f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "class logUniform_gen(rv_continuous):\n",
    "    def _cdf(self, x: float) -> float:\n",
    "        return np.log(x / self.a) / np.log(self.b / self.a)\n",
    "\n",
    "\n",
    "def log_uniform(a: float = 1.0, b: float = np.exp(1.0)) -> 'logUniform_gen':\n",
    "    return logUniform_gen(a=a, b=b, name='log_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a970d3-ba18-4e03-9060-0dc8c29faabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IS_sharpe_ratio(clf: Any) -> float:\n",
    "    best_estimator_ind = np.argmin(clf.cv_results_['rank_test_score'])\n",
    "    mean_score = clf.cv_results_['mean_test_score'][best_estimator_ind]\n",
    "    std_score = clf.cv_results_['std_test_score'][best_estimator_ind]\n",
    "    if mean_score < 0:\n",
    "        return -mean_score / std_score\n",
    "    else:\n",
    "        return mean_score / std_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6537553-7039-46e2-9e34-1188e5352f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
