{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4864b0a3-2caa-419f-ae38-555e8cdfc8e4",
   "metadata": {},
   "source": [
    "# Hyper-parameter Tuning with Cross-Validation\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6bdb42e-198c-420c-833d-dbe5b7fe2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomness\n",
    "import random\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "%matplotlib inline\n",
    "\n",
    "# Date & Time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Typing\n",
    "from typing import Tuple, List, Dict, Union, Optional, Any, Generator\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, plot_roc_curve\n",
    "\n",
    "# Scientific Statistical Python\n",
    "from scipy.stats import jarque_bera\n",
    "from scipy.stats import rv_continuous, kstest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40da1b8-2f2c-4d11-8d0a-b68b05c9be74",
   "metadata": {},
   "source": [
    "### Grid-Search Cross-Validation\n",
    "\n",
    "#### Purged K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f9ec62a-a553-46e1-99e1-b9ccbf81b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPipeline(Pipeline):\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, sample_weight: Optional[pd.Series] = None, **fit_params) -> 'MyPipeline':\n",
    "        if sample_weight is not None:\n",
    "            fit_params[self.steps[-1][0] + '__sample_weight'] = sample_weight\n",
    "        return super().fit(X, y, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02351a69-eb99-4389-9bd5-2f06c3f036e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_hyper_fit_base(\n",
    "    feat: pd.DataFrame, lbl: pd.Series, t1: pd.Series, pipe_clf: Any, param_grid: Dict[str, list],\n",
    "    cv: int = 3, bagging: list = [0, None, 1.0], n_jobs: int = -1, pctEmbargo: float = 0.0, **fit_params) -> Any:\n",
    "   \n",
    "    if set(lbl.values) == {0, 1}:\n",
    "        scoring='f1'    # f1 for meta-labeling\n",
    "    else:\n",
    "        scoring='neg_log_loss'    # symmetric towards all cases\n",
    "    inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)    # purged\n",
    "    gs=GridSearchCV(estimator=pipe_clf ,param_grid=param_grid, scoring=scoring, cv=inner_cv, n_jobs=n_jobs)\n",
    "    gs = gs.fit(feat, lbl, **fit_params).best_estimator_    # pipeline\n",
    "    if bagging[1] is not None and bagging[1] > 0:\n",
    "        gs = BaggingClassifier(base_estimator=MyPipeline(gs.steps), n_estimators=int(bagging[0]),\n",
    "                               max_samples=float(bagging[1]), max_features=float(bagging[2]), n_jobs=n_jobs)\n",
    "        gs = gs.fit(feat, lbl, sample_weight=fit_params[gs.base_estimator.steps[-1][0]+'__sample_weight'])\n",
    "        gs = Pipeline([('bag', gs)])\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2387bf-b2fa-42ff-9e0b-029ef216b91b",
   "metadata": {},
   "source": [
    "#### Randomized Search with Purged K-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e104781-7228-484a-a823-d3564a4f37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_hyper_fit(\n",
    "    feat: pd.DataFrame, lbl: pd.Series, t1: pd.Series, pipe_clf: Any, param_grid: Dict[str, list],\n",
    "    cv: int = 3, bagging: list = [0, None, 1.0], rndSearchIter: int = 0,\n",
    "    n_jobs: int = -1, pctEmbargo: float = 0.0, **fit_params) -> Any:\n",
    "    \n",
    "    if set(lbl.values) == {0, 1}:\n",
    "        scoring='f1'    # f1 for meta-labeling\n",
    "    else:\n",
    "        scoring='neg_log_loss'    # symmetric towards all cases\n",
    "    inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)    # purged\n",
    "    \n",
    "    if rndSearchIter == 0:\n",
    "        gs = GridSearchCV(estimator=pipe_clf, param_grid=param_grid, scoring=scoring, cv=inner_cv, n_jobs=n_jobs)\n",
    "    else:\n",
    "        gs = RandomizedSearchCV(estimator=pipe_clf, param_distributions=param_grid, scoring=scoring,\n",
    "                                cv=inner_cv, n_jobs=n_jobs, n_iter=rndSearchIter)\n",
    "    gs = gs.fit(feat, lbl, **fit_params).best_estimator_    # pipeline\n",
    "    \n",
    "    if bagging[1] is not None and bagging[1] > 0:\n",
    "        gs = BaggingClassifier(base_estimator=MyPipeline(gs.steps), n_estimators=int(bagging[0]),\n",
    "                               max_samples=float(bagging[1]), max_features=float(bagging[2]), n_jobs=n_jobs)\n",
    "        gs = gs.fit(feat, lbl, sample_weight=fit_params[gs.base_estimator.steps[-1][0]+'__sample_weight'])\n",
    "        gs = Pipeline([('bag', gs)])\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b8cf3-e8ba-4072-8c0f-c8b2a8d478a5",
   "metadata": {},
   "source": [
    "#### The `logUniform_gen` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "759920c6-84fe-4840-aac6-5b6810722b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class logUniform_gen(rv_continuous):\n",
    "    def _cdf(self, x: float) -> float:\n",
    "        return np.log(x / self.a) / np.log(self.b / self.a)\n",
    "\n",
    "\n",
    "def log_uniform(a: float = 1.0, b: float = np.exp(1.0)) -> 'logUniform_gen':\n",
    "    return logUniform_gen(a=a, b=b, name='log_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54a970d3-ba18-4e03-9060-0dc8c29faabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IS_sharpe_ratio(clf: Any) -> float:\n",
    "    best_estimator_ind = np.argmin(clf.cv_results_['rank_test_score'])\n",
    "    mean_score = clf.cv_results_['mean_test_score'][best_estimator_ind]\n",
    "    std_score = clf.cv_results_['std_test_score'][best_estimator_ind]\n",
    "    if mean_score < 0:\n",
    "        return -mean_score / std_score\n",
    "    else:\n",
    "        return mean_score / std_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6537553-7039-46e2-9e34-1188e5352f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
